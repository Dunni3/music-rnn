{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 2,
>>>>>>> 0c0b953fc1878ff98be523eb4ba745393e0274ac
   "id": "ce83b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fluidsynth\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pretty_midi\n",
    "import collections\n",
    "from pathlib import Path\n",
    "from IPython import display\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f6d093",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# scrape time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fec86df4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bs4'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nz/hq7gd5f970qb44lzzlb17n0m0000gr/T/ipykernel_85712/2949162298.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0m_datadir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/classical'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_artist_link\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bs4'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "_datadir = Path('./data/classical')\n",
    "def get_artist_link(tag):\n",
    "    if tag.name != 'a':\n",
    "        return False\n",
    "    if tag.parent.name != 'td':\n",
    "        return False\n",
    "    if not tag.parent.has_attr('class'):\n",
    "        return False\n",
    "    if tag.parent.attrs['class'][0] == 'midi':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_files(tag):\n",
    "    if tag.name != 'a':\n",
    "        return False\n",
    "    if tag.parent.name != 'td':\n",
    "        return False\n",
    "    if not tag.parent.has_attr('class'):\n",
    "        return False\n",
    "    if tag.parent.attrs['class'][0] != 'midi':\n",
    "        return False\n",
    "    if tag.has_attr('class'):\n",
    "        return False\n",
    "    href = tag.attrs['href']\n",
    "    if href.split('/')[0] != 'midis':\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "root_url = 'http://www.piano-midi.de/'\n",
    "base_url = f'{root_url}/midi_files.htm'\n",
    "r = requests.get(base_url)\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "composer_page_link = soup.find_all(get_artist_link)\n",
    "for composer_link in composer_page_link:\n",
    "    composer_url = f'{root_url}/{composer_link.attrs[\"href\"]}'\n",
    "    r = requests.get(composer_url)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser') \n",
    "    file_tags = soup.find_all(get_files)\n",
    "    for file_tag in file_tags:\n",
    "        href = file_tag.attrs['href']\n",
    "        filename = href.split('/')[-1]\n",
    "        file_url = f'{root_url}/{href}'\n",
    "        r = requests.get(file_url)\n",
    "        with open(_datadir / filename, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        print(f'{filename} downloaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af7441b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# load metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ffeddb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>composer</th>\n",
       "      <th>end_time</th>\n",
       "      <th>expected_tempo</th>\n",
       "      <th>16th_note_duration</th>\n",
       "      <th>roll_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/classical_C/beethoven_les_adieux_3_C_.mid</td>\n",
       "      <td>beethoven</td>\n",
       "      <td>300.601217</td>\n",
       "      <td>172.650970</td>\n",
       "      <td>0.173761</td>\n",
       "      <td>1729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/classical_C/clementi_opus36_6_2_C_.mid</td>\n",
       "      <td>clementi</td>\n",
       "      <td>136.598831</td>\n",
       "      <td>230.290440</td>\n",
       "      <td>0.130270</td>\n",
       "      <td>1048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/classical_C/chpn_op10_e12_C_.mid</td>\n",
       "      <td>chpn</td>\n",
       "      <td>138.571344</td>\n",
       "      <td>207.866337</td>\n",
       "      <td>0.072162</td>\n",
       "      <td>1920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/classical_C/chpn-p2_C_.mid</td>\n",
       "      <td>chpn-p2</td>\n",
       "      <td>121.577480</td>\n",
       "      <td>107.696155</td>\n",
       "      <td>0.139281</td>\n",
       "      <td>829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/classical_C/chpn_op35_1_C_.mid</td>\n",
       "      <td>chpn</td>\n",
       "      <td>392.316231</td>\n",
       "      <td>197.124089</td>\n",
       "      <td>0.076094</td>\n",
       "      <td>5155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             file   composer    end_time  \\\n",
       "0  data/classical_C/beethoven_les_adieux_3_C_.mid  beethoven  300.601217   \n",
       "1     data/classical_C/clementi_opus36_6_2_C_.mid   clementi  136.598831   \n",
       "2           data/classical_C/chpn_op10_e12_C_.mid       chpn  138.571344   \n",
       "3                 data/classical_C/chpn-p2_C_.mid    chpn-p2  121.577480   \n",
       "4             data/classical_C/chpn_op35_1_C_.mid       chpn  392.316231   \n",
       "\n",
       "   expected_tempo  16th_note_duration  roll_length  \n",
       "0      172.650970            0.173761         1729  \n",
       "1      230.290440            0.130270         1048  \n",
       "2      207.866337            0.072162         1920  \n",
       "3      107.696155            0.139281          829  \n",
       "4      197.124089            0.076094         5155  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_datadir = Path('./data/classical_C')\n",
    "_metadata_file = _datadir / 'metadata.csv'\n",
    "\n",
    "if not _metadata_file.exists():\n",
    "    files = collections.defaultdict(list)\n",
    "    for filepath in _datadir.glob('*.mid'):\n",
    "        files['file'].append(str(filepath))\n",
    "        composer = filepath.stem.split('_')[0]\n",
    "        files['composer'].append(composer)\n",
    "\n",
    "        pm = pretty_midi.PrettyMIDI(str(filepath))\n",
    "        files['end_time'].append(pm.get_end_time())\n",
    "        \n",
    "        \n",
    "        tempos, probabilities = pm.estimate_tempi()\n",
    "        assert np.isclose(sum(probabilities), 1)\n",
    "        tempo_bpm = np.dot(tempos, probabilities) # expected tempo in beats/min\n",
    "        seconds_per_beat = (1/tempo_bpm)*60\n",
    "        time_sig_denom = pm.time_signature_changes[0].denominator\n",
    "        note_dist = seconds_per_beat / (16 / time_sig_denom)\n",
    "        \n",
    "        files['expected_tempo'].append(tempo_bpm)\n",
    "        files['16th_note_duration'].append(note_dist)\n",
    "        roll = midi_to_pianoroll(str(filepath), sample_dist=note_dist)\n",
    "        files['roll_length'].append(roll.shape[1])\n",
    "\n",
    "    df_meta = pd.DataFrame({ key: np.asarray(val) for key, val in files.items() })\n",
    "    df_meta.to_csv(_metadata_file, index=False)\n",
    "else:\n",
    "    df_meta = pd.read_csv(_metadata_file)\n",
    "    \n",
    "    \n",
    "df_meta.head()"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": 8,
   "id": "9562adc2-d094-40a0-9d59-e004454f4c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    337.000000\n",
       "mean       0.100930\n",
       "std        0.048265\n",
       "min        0.032209\n",
       "25%        0.071719\n",
       "50%        0.084659\n",
       "75%        0.119674\n",
       "max        0.380261\n",
       "Name: 16th_note_duration, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta['16th_note_duration'].describe()"
   ]
  },
  {
=======
>>>>>>> 0c0b953fc1878ff98be523eb4ba745393e0274ac
   "cell_type": "markdown",
   "id": "d9bce397",
   "metadata": {
    "tags": []
   },
   "source": [
    "# converting midi to audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a7fe1e30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sampling rate for audio playback\n",
    "_SAMPLING_RATE = 16000\n",
    "\n",
    "def display_audio(pm: pretty_midi.PrettyMIDI, seconds=30):\n",
    "  waveform = pm.fluidsynth(fs=_SAMPLING_RATE)\n",
    "  # Take a sample of the generated waveform to mitigate kernel resets\n",
    "  waveform_short = waveform[:seconds*_SAMPLING_RATE]\n",
    "  return display.Audio(waveform_short, rate=_SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2ac7dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# converting midi to pianoroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "422c33ef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def midi_to_pianoroll(file, sample_dist=0.02):\n",
    "    pm = pretty_midi.PrettyMIDI(file)\n",
    "    sampling_rate = 1/sample_dist\n",
    "    piano_roll = pm.get_piano_roll(fs=sampling_rate)\n",
    "    return piano_roll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c60996",
   "metadata": {},
   "source": [
    "# define dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5096c744-6a03-48f9-9a4d-780df90a2a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midi_dir.is_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61eb0a4f-e93f-4a0c-b29b-4a19c0c6483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_files(processed_data_dir):\n",
    "\n",
    "    preprocessed_data = collections.defaultdict(list)\n",
    "    for midi_dir in processed_data_dir.glob('*/'):\n",
    "        if not midi_dir.is_dir():\n",
    "            continue\n",
    "        piece_name = midi_dir.name\n",
    "        chord_file_name = f'{piece_name}_right_C_full.npy'\n",
    "        data_file = midi_dir / chord_file_name\n",
    "        if not data_file.exists():\n",
    "            print(data_file)\n",
    "            raise Exception('shit is fucked!!')\n",
    "\n",
    "        # record piece name and the path of the file containing the full chord data\n",
    "        preprocessed_data['right_full_chord_file'].append(str(data_file))\n",
    "        preprocessed_data['piece_name'].append(piece_name)\n",
    "\n",
    "        # compute the size of the file after \n",
    "        chord_roll = np.load(data_file)\n",
    "        preprocessed_data['chord_roll_size'].append(chord_roll.shape[0])\n",
    "\n",
    "    df_preprocess = pd.DataFrame({ key: np.asarray(val) for key, val in preprocessed_data.items() })\n",
    "    return df_preprocess\n",
    "\n",
    "processed_data_dir = Path('./data/chopin_processed_bin')\n",
    "df_preprocess = get_preprocessed_files(processed_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ebef2d21-e6e3-4593-a043-72a5ebe26dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullChord(Dataset):\n",
    "    \n",
    "    def __init__(self, df_meta: pd.DataFrame, seq_length: int = 25, \n",
    "                 batch_size: int = 20, batch_per_file=None):\n",
    "        self.df_meta = df_meta.copy()\n",
    "\n",
    "        self.df_meta['n_batches'] = (self.df_meta['chord_roll_size'] - seq_length )*0.95 // batch_size\n",
    "        self.df_meta['n_batches'] = self.df_meta['n_batches'].astype(int)\n",
    "        file_idx_ends = []\n",
    "        n_batches = self.df_meta['n_batches'].values\n",
    "        file_idx_ends = [n_batches[0]*batch_size - 1]\n",
    "        for batches_in_file in n_batches[1:]:\n",
    "            file_idx_ends.append(batches_in_file*batch_size + file_idx_ends[-1] )\n",
    "\n",
    "        self.df_meta['file_idx_ends'] = file_idx_ends\n",
    "\n",
    "        \n",
    "        self.seq_length = seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_per_file = batch_per_file\n",
    "\n",
    "        if self.batch_per_file is not None:\n",
    "            self.idx_per_file = self.batch_size*self.batch_per_file\n",
    "        else:\n",
    "            self.idx_per_file = None\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.batch_per_file is None:\n",
    "            return int(self.df_meta['n_batches'].sum()*self.batch_size)\n",
    "        else:\n",
    "            return self.df_meta.shape[0]*self.idx_per_file\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.batch_per_file is None:\n",
    "            file_idx_ends = self.df_meta['file_idx_ends'].values\n",
    "            for i in range(len(file_idx_ends)):\n",
    "                if idx < file_idx_ends[i]:\n",
    "                    file_idx = i\n",
    "                    break\n",
    "            if file_idx == 0:\n",
    "                idx_start = 0\n",
    "            else:\n",
    "                idx_start = file_idx_ends[file_idx - 1]\n",
    "            window_idx = int(idx - idx_start)\n",
    "        else:\n",
    "            file_idx = idx // self.idx_per_file\n",
    "            window_idx = idx % self.idx_per_file\n",
    "        \n",
    "        \n",
    "        seq, label = self.get_rolls(file_idx, window_idx)\n",
    "        \n",
    "        seq = torch.from_numpy(seq).float()\n",
    "        label = torch.from_numpy(label).float()\n",
    "        return seq, label\n",
    "    \n",
    "    def get_rolls(self, file_idx, window_idx):\n",
    "        file_path = self.df_meta.iloc[file_idx]['right_full_chord_file']\n",
    "        \n",
    "        roll = np.load(file_path)\n",
    "        roll_window = roll[window_idx:window_idx+self.seq_length+1, :]\n",
    "        \n",
    "        seq = roll_window[:-1]\n",
    "        label = roll_window[-1]\n",
    "        return seq, label\n",
    "    \n",
    "    \n",
    "class ChordLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size=1848):\n",
    "        super(ChordLSTM, self).__init__()\n",
    "            \n",
    "        hidden_size = vocab_size // 8\n",
    "            \n",
    "        self.lstm = nn.LSTM(input_size=vocab_size, batch_first=True, num_layers=1, hidden_size=hidden_size)\n",
    "        \n",
    "        self.predict_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, vocab_size),\n",
    "            nn.Softmax(dim=2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        output, (h_n, c_n) = self.lstm(x)\n",
    "         \n",
    "        output = self.predict_layer(h_n)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dd423099-870a-483b-a787-4a57d7535573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_idx = 0\n",
      "iter_idx = 1\n",
      "iter_idx = 2\n",
      "iter_idx = 3\n",
      "iter_idx = 4\n",
      "iter_idx = 5\n",
      "iter_idx=5, iter=5\n",
      "iter_idx=5, train_loss=7.521861394246419\n",
      "iter_idx=5, test_loss=7.52184399843216\n",
      "iter_idx=5, frac_frames_correct=0.0\n",
      "iter_idx = 6\n",
      "iter_idx = 7\n",
      "iter_idx = 8\n",
      "iter_idx = 9\n",
      "iter_idx = 10\n",
      "iter_idx=10, iter=10\n",
      "iter_idx=10, train_loss=7.52182788848877\n",
      "iter_idx=10, test_loss=7.521803941726684\n",
      "iter_idx=10, frac_frames_correct=0.19062499701976776\n",
      "iter_idx = 11\n",
      "iter_idx = 12\n",
      "iter_idx = 13\n",
      "iter_idx = 14\n",
      "iter_idx = 15\n",
      "iter_idx=15, iter=15\n",
      "iter_idx=15, train_loss=7.521801853179932\n",
      "iter_idx=15, test_loss=7.521310896873474\n",
      "iter_idx=15, frac_frames_correct=0.19062499701976776\n",
      "iter_idx = 16\n",
      "iter_idx = 17\n",
      "iter_idx = 18\n",
      "iter_idx = 19\n",
      "iter_idx = 20\n",
      "iter_idx=20, iter=20\n",
      "iter_idx=20, train_loss=7.516920948028565\n",
      "iter_idx=20, test_loss=7.49155084848404\n",
      "iter_idx=20, frac_frames_correct=0.19062499701976776\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "learning_rate = 1e-3\n",
    "batch_size = 8\n",
    "num_workers = 0\n",
    "\n",
    "processed_data_dir = Path('./data/chopin_processed_bin')\n",
    "df_preprocess = get_preprocessed_files(processed_data_dir)\n",
    "\n",
    "rng = np.random.default_rng(12345)\n",
    "idx = np.arange(df_preprocess.shape[0])\n",
    "n_train = int(0.8*idx.shape[0])\n",
    "train_idx = rng.choice(idx, size=n_train, replace=False)\n",
    "test_idx = idx[~np.in1d(idx, train_idx)]\n",
    "df_train = df_preprocess.iloc[train_idx]\n",
    "df_test = df_preprocess.iloc[test_idx]\n",
    "\n",
    "dset_train = FullChord(df_meta=df_train, \n",
    "                    batch_size=batch_size,\n",
    "                    batch_per_file=None,\n",
    "                    seq_length=seq_length)\n",
    "\n",
    "\n",
    "dset_test = FullChord(df_meta=df_test, \n",
    "                    batch_size=batch_size, \n",
    "                    batch_per_file=20,\n",
    "                    seq_length=seq_length)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(dset_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_dataloader = DataLoader(dset_test, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "model = ChordLSTM()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "metrics = collections.defaultdict(list)\n",
    "\n",
    "iter_idx = -1\n",
    "train_iterator = iter(train_dataloader)\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "while iter_idx < 20:\n",
    "    iter_idx += 1\n",
    "    print(f'iter_idx = {iter_idx}', flush=True)\n",
    "\n",
    "    try:\n",
    "        features, labels = next(train_iterator)\n",
    "    except StopIteration:\n",
    "        train_iterator = iter(train_dataloader)\n",
    "        features, labels = next(train_iterator)\n",
    "\n",
    "\n",
    "    # compute prediction and loss\n",
    "    pred = model(features)[0, :, :]\n",
    "    loss = loss_fn(pred, labels)\n",
    "\n",
    "    # backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "\n",
    "    # compute metrics every 10 iterations\n",
    "    if iter_idx > 0 and iter_idx % 5 == 0:\n",
    "\n",
    "        metrics['iter'].append(iter_idx)\n",
    "\n",
    "        # compute train loss\n",
    "        train_loss = np.mean(np.asarray(train_losses))\n",
    "        metrics['train_loss'].append(train_loss)\n",
    "        train_losses = []\n",
    "\n",
    "\n",
    "        # test loop\n",
    "        test_loss_fn = nn.CrossEntropyLoss()\n",
    "        test_loss = 0\n",
    "        frames_correct = 0\n",
    "        num_batches = len(test_dataloader)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for features, labels in test_dataloader:\n",
    "                pred = model(features)[0, :, :]\n",
    "                test_loss += test_loss_fn(pred, labels).item()\n",
    "\n",
    "                pred_chords = pred.argmax(axis=1)\n",
    "                label_chords = labels.argmax(axis=1) \n",
    "                equal = torch.eq(pred_chords, label_chords)\n",
    "                frames_correct += torch.sum(equal)\n",
    "        model.train()\n",
    "\n",
    "        frac_frames_correct = frames_correct / (num_batches*batch_size)\n",
    "        avg_test_loss = test_loss / num_batches\n",
    "\n",
    "        metrics['test_loss'].append(avg_test_loss)\n",
    "        metrics['frac_frames_correct'].append(frac_frames_correct)\n",
    "\n",
    "        # save metrics\n",
    "        df_metrics = pd.DataFrame({ key: np.asarray(val) for key, val in metrics.items() })\n",
    "        # df_metrics.to_csv(metrics_file)\n",
    "\n",
    "        for key, val in metrics.items():\n",
    "            print(f'iter_idx={iter_idx}, {key}={val[-1]}')\n",
    "\n",
    "        # save model\n",
    "        # state_file = output_dir / f'model_weights_iter{iter_idx}.pth'\n",
    "        # torch.save(model.state_dict(), state_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a2ec6114-e2a0-4763-b8b6-6fa6883d15b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iter</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>frac_frames_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>7.521852</td>\n",
       "      <td>7.521848</td>\n",
       "      <td>0.003125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>7.521850</td>\n",
       "      <td>7.521825</td>\n",
       "      <td>0.001875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>7.521820</td>\n",
       "      <td>7.521770</td>\n",
       "      <td>0.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>7.521413</td>\n",
       "      <td>7.519488</td>\n",
       "      <td>0.190625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   iter  train_loss  test_loss  frac_frames_correct\n",
       "0     5    7.521852   7.521848             0.003125\n",
       "1    10    7.521850   7.521825             0.001875\n",
       "2    15    7.521820   7.521770             0.190000\n",
       "3    20    7.521413   7.519488             0.190625"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b1c50bc-fa7f-43fb-a428-4551161227bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-38a2ba01143f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "for features, labels in iter(test_dataloader):\n",
    "    model(features).argmax(axis=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
